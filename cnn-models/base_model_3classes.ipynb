{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all-data-processed-3classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Clean sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The utility will also provide services related...</td>\n",
       "      <td>utility also provide services related electric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Niam offer financial investors a high return v...</td>\n",
       "      <td>niam offer financial investors high return via...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>Pinduoduo's stock plunges 23% premarket on hea...</td>\n",
       "      <td>pinduoduos stock plunges 23 premarket heavy vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Azeri Snap Elections Condemned by Monitors for...</td>\n",
       "      <td>azeri snap elections condemned monitors vote v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>The #market is seeing strong #tax-loss selling...</td>\n",
       "      <td>market seeing strong taxloss selling investors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31864</th>\n",
       "      <td>0</td>\n",
       "      <td>The U.S. Is Firing Blanks Against a New Irania...</td>\n",
       "      <td>yous firing blanks new iranian threat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31865</th>\n",
       "      <td>0</td>\n",
       "      <td>Storengy is the GDF SUEZ company that is dedic...</td>\n",
       "      <td>storengy gdf suez company dedicated undergroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31866</th>\n",
       "      <td>1</td>\n",
       "      <td>FDA approves Aquestive's ALS treatment https:...</td>\n",
       "      <td>fda approves aquestives als treatment httpstco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31867</th>\n",
       "      <td>0</td>\n",
       "      <td>SoftBank to Create Japan Internet Giant to Bat...</td>\n",
       "      <td>softbank create japan internet giant battle gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31868</th>\n",
       "      <td>0</td>\n",
       "      <td>Marriott’s Mission: Make W Hotels Cool Again</td>\n",
       "      <td>marriotts mission make w hotels cool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31869 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentiment                                           Sentence  \\\n",
       "0              0  The utility will also provide services related...   \n",
       "1              0  Niam offer financial investors a high return v...   \n",
       "2             -1  Pinduoduo's stock plunges 23% premarket on hea...   \n",
       "3              0  Azeri Snap Elections Condemned by Monitors for...   \n",
       "4             -1  The #market is seeing strong #tax-loss selling...   \n",
       "...          ...                                                ...   \n",
       "31864          0  The U.S. Is Firing Blanks Against a New Irania...   \n",
       "31865          0  Storengy is the GDF SUEZ company that is dedic...   \n",
       "31866          1   FDA approves Aquestive's ALS treatment https:...   \n",
       "31867          0  SoftBank to Create Japan Internet Giant to Bat...   \n",
       "31868          0       Marriott’s Mission: Make W Hotels Cool Again   \n",
       "\n",
       "                                         Clean sentences  \n",
       "0      utility also provide services related electric...  \n",
       "1      niam offer financial investors high return via...  \n",
       "2      pinduoduos stock plunges 23 premarket heavy vo...  \n",
       "3      azeri snap elections condemned monitors vote v...  \n",
       "4      market seeing strong taxloss selling investors...  \n",
       "...                                                  ...  \n",
       "31864              yous firing blanks new iranian threat  \n",
       "31865  storengy gdf suez company dedicated undergroun...  \n",
       "31866  fda approves aquestives als treatment httpstco...  \n",
       "31867  softbank create japan internet giant battle gl...  \n",
       "31868               marriotts mission make w hotels cool  \n",
       "\n",
       "[31869 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Clean sentences\"] = data[\"Clean sentences\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28682,), (3187,), (28682,), (3187,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"Clean sentences\"], data[\"Sentiment\"], test_size=0.1, random_state=42)\n",
    "X_train.shape , X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing (str to int conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911      mjardin group reports q3 results httpstco1owwk...\n",
       "10405    wells fargo downgrades netflix underperform se...\n",
       "28158    kohls shares plunge cramer calls ceo glossing ...\n",
       "25140    line 4 run fully underground comprise 10 stati...\n",
       "12255    uponor made operating profit eur 1510 mn eur 1...\n",
       "                               ...                        \n",
       "29802    recent rally could bear market trap says mille...\n",
       "5390     disney downgraded analyst says parks attendanc...\n",
       "860      extraordinary general meeting expected take pl...\n",
       "15795          energy settlement prices httpstcoy70kosce0a\n",
       "23654               resulted improved sales figures sweden\n",
       "Name: Clean sentences, Length: 28682, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=29081\n",
      "Number of Documents=28682\n"
     ]
    }
   ],
   "source": [
    "vocab = len(token.index_word) + 1\n",
    "print(\"Vocabulary size={}\".format(len(token.word_index)))\n",
    "print(\"Number of Documents={}\".format(token.document_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = token.texts_to_sequences(X_train)\n",
    "X_test = token.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [len(seq) for seq in X_train]\n",
    "\n",
    "# Find the maximum length\n",
    "max_length = max(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28682, 47), (3187, 47))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 47\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "911      0\n",
       "10405   -1\n",
       "28158   -1\n",
       "25140    0\n",
       "12255    1\n",
       "        ..\n",
       "29802   -1\n",
       "5390    -1\n",
       "860      0\n",
       "15795    0\n",
       "23654    1\n",
       "Name: Sentiment, Length: 28682, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "y_train_labels = np.array(y_train)  # Convert to NumPy array if not already\n",
    "y_test_labels = np.array(y_test)  # Convert to NumPy array if not already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11997,    20,    55,   122,    39, 15261,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train_labels == -1, 0, y_train)\n",
    "y_train = np.where(y_train_labels == 0, 1, y_train)\n",
    "y_train = np.where(y_train_labels == 1, 2, y_train)\n",
    "\n",
    "y_test = np.where(y_test_labels == -1, 0, y_test)\n",
    "y_test = np.where(y_test_labels == 0, 1, y_test)\n",
    "y_test = np.where(y_test_labels == 1, 2, y_test)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.9\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 47, 300)           8724600   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 40, 64)            153664    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 40, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 20, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20, 64)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20, 8)             520       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 20, 8)            32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 20, 8)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20, 4)             36        \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 20, 4)             0         \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 4)                0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 15        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,879,123\n",
      "Trainable params: 8,878,979\n",
      "Non-trainable params: 144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "vec_size = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(token.index_word) + 1, vec_size, input_length=max_length))\n",
    "model.add(Conv1D(64, 8, activation=\"relu\"))\n",
    "model.add(BatchNormalization())  # Add BatchNormalization\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(BatchNormalization())  # Add BatchNormalization\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer with softmax activation\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897/897 [==============================] - 116s 128ms/step - loss: 1.0996 - accuracy: 0.4520 - val_loss: 0.7555 - val_accuracy: 0.6574 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "897/897 [==============================] - 112s 125ms/step - loss: 0.6607 - accuracy: 0.7004 - val_loss: 0.4513 - val_accuracy: 0.8237 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "897/897 [==============================] - 118s 131ms/step - loss: 0.4160 - accuracy: 0.8303 - val_loss: 0.3329 - val_accuracy: 0.8830 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "897/897 [==============================] - 111s 124ms/step - loss: 0.2817 - accuracy: 0.8959 - val_loss: 0.2757 - val_accuracy: 0.8965 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "897/897 [==============================] - 115s 128ms/step - loss: 0.2014 - accuracy: 0.9272 - val_loss: 0.2400 - val_accuracy: 0.9056 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "897/897 [==============================] - 113s 126ms/step - loss: 0.1506 - accuracy: 0.9462 - val_loss: 0.2153 - val_accuracy: 0.9190 - lr: 9.0000e-05\n",
      "Epoch 7/20\n",
      "897/897 [==============================] - 104s 116ms/step - loss: 0.1176 - accuracy: 0.9587 - val_loss: 0.2052 - val_accuracy: 0.9203 - lr: 8.1000e-05\n",
      "Epoch 8/20\n",
      "897/897 [==============================] - 104s 115ms/step - loss: 0.0987 - accuracy: 0.9647 - val_loss: 0.1884 - val_accuracy: 0.9316 - lr: 7.2900e-05\n",
      "Epoch 9/20\n",
      "897/897 [==============================] - 102s 113ms/step - loss: 0.0769 - accuracy: 0.9738 - val_loss: 0.2177 - val_accuracy: 0.9194 - lr: 6.5610e-05\n",
      "Epoch 10/20\n",
      "897/897 [==============================] - 115s 128ms/step - loss: 0.0660 - accuracy: 0.9771 - val_loss: 0.2093 - val_accuracy: 0.9234 - lr: 5.9049e-05\n",
      "Epoch 11/20\n",
      "897/897 [==============================] - 103s 115ms/step - loss: 0.0574 - accuracy: 0.9801 - val_loss: 0.1908 - val_accuracy: 0.9332 - lr: 5.3144e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "# Define EarlyStopping and ModelCheckpoint callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=32, callbacks=[early_stopping, model_checkpoint, lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1884 - accuracy: 0.9316\n",
      "Test Loss: 0.18837913870811462\n",
      "Test Accuracy: 0.931597113609314\n",
      "100/100 [==============================] - 1s 6ms/step\n",
      "Predicted probabilities: [[1.3037772e-02 9.8172981e-01 5.2323430e-03]\n",
      " [3.6761787e-05 1.5095241e-03 9.9845368e-01]\n",
      " [9.9885881e-01 3.9961856e-04 7.4158580e-04]\n",
      " [2.0378898e-03 9.8916948e-01 8.7927384e-03]\n",
      " [9.2778450e-01 6.6224545e-02 5.9910351e-03]\n",
      " [4.8702490e-02 8.5636389e-01 9.4933599e-02]\n",
      " [3.6684325e-04 1.0322688e-02 9.8931050e-01]\n",
      " [4.4437833e-02 7.5663322e-01 1.9892897e-01]\n",
      " [8.4159747e-03 9.1958508e-02 8.9962548e-01]\n",
      " [4.5863027e-03 8.7943763e-01 1.1597610e-01]]\n",
      "Actual labels: [[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "                                                Text  Actual Class  \\\n",
      "0  international leads consumer gainers nio tata ...             1   \n",
      "1         pleased bjorn wahlroos accepted nomination             2   \n",
      "2  us energy information administration projects ...             0   \n",
      "3  edited transcript earnings conference call pre...             1   \n",
      "4  okmetic expects net sales first half 2009 less...             0   \n",
      "5  universities pledged set questions hit strike ...             1   \n",
      "6            company market share continued increase             2   \n",
      "7  mhi selected inclusion four esg investment ind...             1   \n",
      "8       stocks dow 70483 points nasdaq 16539 sp 7215             2   \n",
      "9  ms responsible hkscan hr functions development...             1   \n",
      "\n",
      "   Predicted Class                   Predicted Probabilities  \n",
      "0                1     [0.013037772, 0.9817298, 0.005232343]  \n",
      "1                2  [3.6761787e-05, 0.0015095241, 0.9984537]  \n",
      "2                0  [0.9988588, 0.00039961856, 0.0007415858]  \n",
      "3                1    [0.0020378898, 0.9891695, 0.008792738]  \n",
      "4                0     [0.9277845, 0.066224545, 0.005991035]  \n",
      "5                1        [0.04870249, 0.8563639, 0.0949336]  \n",
      "6                2   [0.00036684325, 0.010322688, 0.9893105]  \n",
      "7                1      [0.044437833, 0.7566332, 0.19892897]  \n",
      "8                2      [0.008415975, 0.09195851, 0.8996255]  \n",
      "9                1      [0.0045863027, 0.8794376, 0.1159761]  \n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Get predicted probabilities\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print some example predictions\n",
    "print(\"Predicted probabilities:\", predictions[:10])\n",
    "print(\"Actual labels:\", y_test[:10])\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels for comparison\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Create a DataFrame to compare predictions with actual values\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': [\" \".join([token.index_word.get(idx, \"\") for idx in x if idx != 0]) for x in X_test],\n",
    "    'Actual Class': true_classes,\n",
    "    'Predicted Class': np.argmax(predictions, axis=1),\n",
    "    'Predicted Probabilities': list(predictions)\n",
    "})\n",
    "\n",
    "# Show the DataFrame\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['sentiment_score'] = results_df['Predicted Probabilities'].apply(lambda x: -1 * x[0] + 1 * x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel(\"scores.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sentimentModel.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(token, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
