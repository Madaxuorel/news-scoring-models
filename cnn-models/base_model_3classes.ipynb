{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all-data-processed-3classes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Clean sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>In Finland 's Hobby Hall 's sales decreased by...</td>\n",
       "      <td>finland hobby hall sales decreased 10 internat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Panostaja did not disclose the purchase price .</td>\n",
       "      <td>panostaja disclose purchase price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>Scanfil will execute the temporary lay-offs by...</td>\n",
       "      <td>scanfil execute temporary layoffs midoctober 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>operating profit rose eur 131 mn eur 87 mn cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>The company 's share is quoted on NASDAQ OMX H...</td>\n",
       "      <td>company share quoted nasdaq omx helsinki rauta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8632</th>\n",
       "      <td>0</td>\n",
       "      <td>In addition to the presentations held by Presi...</td>\n",
       "      <td>addition presentations held president ceo kai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8633</th>\n",
       "      <td>1</td>\n",
       "      <td>However , the broker gave an `` outperform '' ...</td>\n",
       "      <td>however broker gave outperform recommendation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8634</th>\n",
       "      <td>0</td>\n",
       "      <td>Tampere Science Parks is a Finnish company tha...</td>\n",
       "      <td>tampere science parks finnish company owns lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8635</th>\n",
       "      <td>0</td>\n",
       "      <td>Aldata noted that its Voice Supply Chain Techn...</td>\n",
       "      <td>aldata noted voice supply chain technology app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8636</th>\n",
       "      <td>1</td>\n",
       "      <td>In the reporting period , the company 's opera...</td>\n",
       "      <td>reporting period company operating profit grew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8637 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                           Sentence  \\\n",
       "0            -1  In Finland 's Hobby Hall 's sales decreased by...   \n",
       "1             0    Panostaja did not disclose the purchase price .   \n",
       "2            -1  Scanfil will execute the temporary lay-offs by...   \n",
       "3             1  Operating profit rose to EUR 13.1 mn from EUR ...   \n",
       "4             0  The company 's share is quoted on NASDAQ OMX H...   \n",
       "...         ...                                                ...   \n",
       "8632          0  In addition to the presentations held by Presi...   \n",
       "8633          1  However , the broker gave an `` outperform '' ...   \n",
       "8634          0  Tampere Science Parks is a Finnish company tha...   \n",
       "8635          0  Aldata noted that its Voice Supply Chain Techn...   \n",
       "8636          1  In the reporting period , the company 's opera...   \n",
       "\n",
       "                                        Clean sentences  \n",
       "0     finland hobby hall sales decreased 10 internat...  \n",
       "1                     panostaja disclose purchase price  \n",
       "2     scanfil execute temporary layoffs midoctober 2...  \n",
       "3     operating profit rose eur 131 mn eur 87 mn cor...  \n",
       "4     company share quoted nasdaq omx helsinki rauta...  \n",
       "...                                                 ...  \n",
       "8632  addition presentations held president ceo kai ...  \n",
       "8633  however broker gave outperform recommendation ...  \n",
       "8634  tampere science parks finnish company owns lea...  \n",
       "8635  aldata noted voice supply chain technology app...  \n",
       "8636  reporting period company operating profit grew...  \n",
       "\n",
       "[8637 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7773,), (864,), (7773,), (864,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"Clean sentences\"], data[\"Sentiment\"], test_size=0.1, random_state=42)\n",
    "X_train.shape , X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizing (str to int conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4655    manager critical politicians failure different...\n",
       "7959    russia ready participate open tender latteleco...\n",
       "37      developments partly reflect government higher ...\n",
       "5761    sampo group become major shareholder nordea ow...\n",
       "8605    seawind en route finnish port turku stockholm ...\n",
       "                              ...                        \n",
       "5734    webcast may followed online company website ww...\n",
       "5191    according finnish scanfil founder chairman boa...\n",
       "5390    currency conversions based exchange rates time...\n",
       "860     thus method cut working costs fasten planning ...\n",
       "7270    omx helsinki 25 index 092 pct 251867 helsinki ...\n",
       "Name: Clean sentences, Length: 7773, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=10468\n",
      "Number of Documents=7773\n"
     ]
    }
   ],
   "source": [
    "vocab = len(token.index_word) + 1\n",
    "print(\"Vocabulary size={}\".format(len(token.word_index)))\n",
    "print(\"Number of Documents={}\".format(token.document_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = token.texts_to_sequences(X_train)\n",
    "X_test = token.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [len(seq) for seq in X_train]\n",
    "\n",
    "# Find the maximum length\n",
    "max_length = max(sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7773, 47), (864, 47))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 47\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4655    0\n",
       "7959    0\n",
       "37      0\n",
       "5761    0\n",
       "8605   -1\n",
       "       ..\n",
       "5734    0\n",
       "5191    0\n",
       "5390    0\n",
       "860     1\n",
       "7270    1\n",
       "Name: Sentiment, Length: 7773, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "y_train_labels = np.array(y_train)  # Convert to NumPy array if not already\n",
    "y_test_labels = np.array(y_test)  # Convert to NumPy array if not already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 559, 4936, 6357, 6358, 6359, 6360,  560,   54, 1784, 6361,  583,\n",
       "       6362, 1524,  929, 2460,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train_labels == -1, 0, y_train)\n",
    "y_train = np.where(y_train_labels == 0, 1, y_train)\n",
    "y_train = np.where(y_train_labels == 1, 2, y_train)\n",
    "\n",
    "y_test = np.where(y_test_labels == -1, 0, y_test)\n",
    "y_test = np.where(y_test_labels == 0, 1, y_test)\n",
    "y_test = np.where(y_test_labels == 1, 2, y_test)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.9\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 47, 300)           3140700   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 40, 64)            153664    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 40, 64)           256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 20, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 64)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20, 8)             520       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 20, 8)            32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20, 8)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20, 4)             36        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 20, 4)             0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 4)                0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 15        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,295,223\n",
      "Trainable params: 3,295,079\n",
      "Non-trainable params: 144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "vec_size = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(token.index_word) + 1, vec_size, input_length=max_length))\n",
    "model.add(Conv1D(64, 8, activation=\"relu\"))\n",
    "model.add(BatchNormalization())  # Add BatchNormalization\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(BatchNormalization())  # Add BatchNormalization\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer with softmax activation\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "243/243 [==============================] - 13s 47ms/step - loss: 1.3163 - accuracy: 0.4016 - val_loss: 1.0773 - val_accuracy: 0.3414 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "243/243 [==============================] - 11s 44ms/step - loss: 0.9693 - accuracy: 0.5064 - val_loss: 1.0380 - val_accuracy: 0.3877 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "243/243 [==============================] - 10s 43ms/step - loss: 0.7694 - accuracy: 0.6676 - val_loss: 0.7821 - val_accuracy: 0.7523 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "243/243 [==============================] - 10s 39ms/step - loss: 0.6068 - accuracy: 0.8136 - val_loss: 0.6406 - val_accuracy: 0.8009 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "243/243 [==============================] - 10s 42ms/step - loss: 0.4879 - accuracy: 0.8832 - val_loss: 0.5774 - val_accuracy: 0.8229 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "243/243 [==============================] - 10s 42ms/step - loss: 0.4098 - accuracy: 0.9132 - val_loss: 0.4970 - val_accuracy: 0.8553 - lr: 9.0000e-05\n",
      "Epoch 7/20\n",
      "243/243 [==============================] - 11s 47ms/step - loss: 0.3438 - accuracy: 0.9321 - val_loss: 0.4093 - val_accuracy: 0.8993 - lr: 8.1000e-05\n",
      "Epoch 8/20\n",
      "243/243 [==============================] - 10s 41ms/step - loss: 0.2913 - accuracy: 0.9497 - val_loss: 0.3913 - val_accuracy: 0.8993 - lr: 7.2900e-05\n",
      "Epoch 9/20\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.2661 - accuracy: 0.9475 - val_loss: 0.3745 - val_accuracy: 0.9005 - lr: 6.5610e-05\n",
      "Epoch 10/20\n",
      "243/243 [==============================] - 11s 44ms/step - loss: 0.2248 - accuracy: 0.9605 - val_loss: 0.3339 - val_accuracy: 0.9086 - lr: 5.9049e-05\n",
      "Epoch 11/20\n",
      "243/243 [==============================] - 15s 61ms/step - loss: 0.2055 - accuracy: 0.9586 - val_loss: 0.3141 - val_accuracy: 0.9109 - lr: 5.3144e-05\n",
      "Epoch 12/20\n",
      "243/243 [==============================] - 10s 41ms/step - loss: 0.1799 - accuracy: 0.9650 - val_loss: 0.3303 - val_accuracy: 0.9028 - lr: 4.7830e-05\n",
      "Epoch 13/20\n",
      "243/243 [==============================] - 10s 40ms/step - loss: 0.1544 - accuracy: 0.9722 - val_loss: 0.2798 - val_accuracy: 0.9155 - lr: 4.3047e-05\n",
      "Epoch 14/20\n",
      "243/243 [==============================] - 10s 41ms/step - loss: 0.1440 - accuracy: 0.9711 - val_loss: 0.2973 - val_accuracy: 0.9109 - lr: 3.8742e-05\n",
      "Epoch 15/20\n",
      "243/243 [==============================] - 10s 43ms/step - loss: 0.1315 - accuracy: 0.9761 - val_loss: 0.2553 - val_accuracy: 0.9213 - lr: 3.4868e-05\n",
      "Epoch 16/20\n",
      "243/243 [==============================] - 10s 42ms/step - loss: 0.1180 - accuracy: 0.9786 - val_loss: 0.2692 - val_accuracy: 0.9178 - lr: 3.1381e-05\n",
      "Epoch 17/20\n",
      "243/243 [==============================] - 10s 42ms/step - loss: 0.1123 - accuracy: 0.9771 - val_loss: 0.2733 - val_accuracy: 0.9155 - lr: 2.8243e-05\n",
      "Epoch 18/20\n",
      "243/243 [==============================] - 14s 56ms/step - loss: 0.1034 - accuracy: 0.9807 - val_loss: 0.2542 - val_accuracy: 0.9167 - lr: 2.5419e-05\n",
      "Epoch 19/20\n",
      "243/243 [==============================] - 13s 53ms/step - loss: 0.0958 - accuracy: 0.9829 - val_loss: 0.2646 - val_accuracy: 0.9144 - lr: 2.2877e-05\n",
      "Epoch 20/20\n",
      "243/243 [==============================] - 12s 48ms/step - loss: 0.0978 - accuracy: 0.9785 - val_loss: 0.2645 - val_accuracy: 0.9132 - lr: 2.0589e-05\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "# Define EarlyStopping and ModelCheckpoint callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=32, callbacks=[early_stopping, model_checkpoint, lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 7ms/step - loss: 0.2645 - accuracy: 0.9132\n",
      "Test Loss: 0.26447364687919617\n",
      "Test Accuracy: 0.9131944179534912\n",
      "27/27 [==============================] - 0s 8ms/step\n",
      "Predicted probabilities: [[3.90894786e-02 9.51089025e-01 9.82153788e-03]\n",
      " [9.89223838e-01 5.77101298e-03 5.00515010e-03]\n",
      " [4.34361249e-02 3.37353005e-04 9.56226528e-01]\n",
      " [9.88966286e-01 9.76068527e-03 1.27302855e-03]\n",
      " [3.75201441e-02 9.17613983e-01 4.48658541e-02]\n",
      " [9.14939865e-02 5.68979502e-01 3.39526534e-01]\n",
      " [5.51021732e-02 8.43448102e-01 1.01449735e-01]\n",
      " [1.99963689e-01 4.37835723e-01 3.62200558e-01]\n",
      " [9.83310878e-01 1.38064167e-02 2.88272835e-03]\n",
      " [6.13157712e-02 9.14170802e-01 2.45134234e-02]]\n",
      "Actual labels: [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "                                                Text  Actual Class  \\\n",
      "0  pharmaceutical market belgium global research ...             1   \n",
      "1  repeats sees 2008 operating profit yy reportin...             0   \n",
      "2    department store sales improved 14 eur 10706 mn             2   \n",
      "3      company initially estimated would cut 30 jobs             0   \n",
      "4  china unicom nyse second largest mobile carrie...             1   \n",
      "5  fiskars also engaged global supply marine ener...             1   \n",
      "6  however production almost entirely labor inten...             1   \n",
      "7  ruukki restructuring commercial industrial con...             1   \n",
      "8  finnish solutions provider affecto oyj hel afe...             0   \n",
      "9  componenta also offer fiveyear subordinated lo...             1   \n",
      "\n",
      "   Predicted Class                 Predicted Probabilities  \n",
      "0                1     [0.03908948, 0.951089, 0.009821538]  \n",
      "1                0   [0.98922384, 0.005771013, 0.00500515]  \n",
      "2                2   [0.043436125, 0.000337353, 0.9562265]  \n",
      "3                0  [0.9889663, 0.009760685, 0.0012730286]  \n",
      "4                1    [0.037520144, 0.917614, 0.044865854]  \n",
      "5                1     [0.09149399, 0.5689795, 0.33952653]  \n",
      "6                1   [0.055102173, 0.8434481, 0.101449735]  \n",
      "7                1    [0.19996369, 0.43783572, 0.36220056]  \n",
      "8                0  [0.9833109, 0.013806417, 0.0028827284]  \n",
      "9                1    [0.06131577, 0.9141708, 0.024513423]  \n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Get predicted probabilities\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print some example predictions\n",
    "print(\"Predicted probabilities:\", predictions[:10])\n",
    "print(\"Actual labels:\", y_test[:10])\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels for comparison\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Create a DataFrame to compare predictions with actual values\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': [\" \".join([token.index_word.get(idx, \"\") for idx in x if idx != 0]) for x in X_test],\n",
    "    'Actual Class': true_classes,\n",
    "    'Predicted Class': np.argmax(predictions, axis=1),\n",
    "    'Predicted Probabilities': list(predictions)\n",
    "})\n",
    "\n",
    "# Show the DataFrame\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['sentiment_score'] = results_df['Predicted Probabilities'].apply(lambda x: -1 * x[0] + 1 * x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel(\"scores.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sentimentModel.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(token, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
